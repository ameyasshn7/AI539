# -*- coding: utf-8 -*-
"""driver_udpos.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13VIkJqXF6SxB5f5l1CT86PzzVJGya0AO
"""

import torch
from torch.utils.data import DataLoader
from torchtext import datasets
from torch.utils.data.backward_compatibility import worker_init_fn

# !pip install portalocker>=2.0.0

# !pip show torch
# !pip show torchtext
# !pip show torchdata
# !pip show portalocker

# Create data pipeline
# train_data = datasets.UDPOS(split='train')
TEXT = Field(tokenize='spacy', tokenizer_language='en_core_web_sm')
UD_TAGS = Field(unk_token=None)  # Universal Dependency Tags
PTB_TAGS = Field(unk_token=None)  # Penn Treebank Tags

fields = (('text', TEXT), ('udtags', UD_TAGS), ('ptbtags', PTB_TAGS))

# Load the dataset
train_data, valid_data, test_data = UDPOS.splits(fields=fields, root='.')

# Build vocabularies
TEXT.build_vocab(train_data, min_freq=3)
UD_TAGS.build_vocab(train_data)
PTB_TAGS.build_vocab(train_data)

# train_data

def pad_collate(batch):
  xx = [b[0] for b in batch]
  yy = [b[1] for b in batch]
  x_lens = [len(x) for x in xx]
  return xx, yy, x_lens

train_loader = DataLoader(dataset=train_data, batch_size=5,
                          shuffle = True, num_workers=1,
                          worker_init_fn=worker_init_fn,
                          drop_last=True, collate_fn=pad_collate)

xx,yy,x_lens=next(iter(train_loader))

def visualizeSentenceWithTags(text,udtags):

  print ( " Token " + " " . join ([ " " ]*(15) ) + " POS Tag " )
  print ( "--------------------------------------------------" )
  for w , t in zip ( text , udtags ) :
    print ( w + " " . join ([ " " ]*(20 - len ( w ) ) ) + t )

visualizeSentenceWithTags(xx[0],yy[0])

#looking at random samples of the data
import random

def visualize_random_samples(data_loader, num_samples=5):
    for i, (xx, yy, x_len) in enumerate(data_loader):
        if i >= num_samples:
            break
        print(f"Sample {i+1}:")
        visualizeSentenceWithTags(xx[0], yy[0])

visualize_random_samples(train_loader)

# Clear the cache!
# !rm -rf /root/.cache/torch/text/datasets/UDPOS

# Python script to test dataset loading
import torchtext

# Assuming the dataset is UDPOS
train_data = torchtext.datasets.UDPOS(split='train')
for item in train_data:
    print(item)
    break  # Just to check the first item

from collections import Counter
import matplotlib.pyplot as plt


position_tags_count = Counter()

for _ , yy , _ in train_loader:
  for tags in yy:
    position_tags_count.update(tags)

plt.figure(figsize=(10,5))
plt.bar(position_tags_count.keys(), position_tags_count.values())
plt.xlabel('Part of Speech Tags')
plt.ylabel('Frequency')
plt.title('Histogram of POS Tags in Dataset')
plt.xticks(rotation=90)
plt.show()

most_common_tag, most_common_tag_count = position_tags_count.most_common(1)[0]
total_tags = sum(position_tags_count.values())
majority_label_accuracy = most_common_tag_count/total_tags

print(f'most common tag is {most_common_tag} with a frequencey of {most_common_tag_count}')
print(f'majority baseline accuracy is {majority_label_accuracy:.2f}')

# !pip install torchtext==0.6

"""Bi-LSTM"""

import torch
import torch.nn as nn
import torch.optim as optim
from torchtext.datasets import UDPOS
from torchtext.vocab import GloVe
import numpy as np
import matplotlib.pyplot as plt
from torchtext.data import Field, BucketIterator

# Define fields
TEXT = Field(tokenize='spacy', tokenizer_language='en_core_web_sm', lower=True, include_lengths=True)
UD_TAGS = Field(unk_token=None, is_target=True)

# Load dataset
fields = (('text', TEXT), ('udtags', UD_TAGS))
train_data, val_data, test_data = UDPOS.splits(fields=fields)

# Build vocabularies
TEXT.build_vocab(train_data, vectors=GloVe(name='6B', dim=100))
UD_TAGS.build_vocab(train_data)

# Create iterators
BATCH_SIZE = 64
train_iterator, valid_iterator, test_iterator = BucketIterator.splits(
    (train_data, val_data, test_data),
    batch_size=BATCH_SIZE,
    sort_within_batch=True,
    device='cuda' if torch.cuda.is_available() else 'cpu')

class BiLSTM(nn.Module):
    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout):
        super().__init__()
        self.embedding = nn.Embedding(input_dim, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout)
        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)
        self.dropout = nn.Dropout(dropout)

    def forward(self, text, text_lengths):
        # text = [sent len, batch size]
        embedded = self.dropout(self.embedding(text))
        # Pack sequence
        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)
        packed_output, _ = self.lstm(packed_embedded)
        # Unpack sequence
        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output)
        # output = [sent len, batch size, hid dim * num directions]
        output = self.fc(self.dropout(output))
        return output

INPUT_DIM = len(TEXT.vocab)
EMBEDDING_DIM = 100
HIDDEN_DIM = 256
OUTPUT_DIM = len(UD_TAGS.vocab)
N_LAYERS = 2
BIDIRECTIONAL = True
DROPOUT = 0.25

model = BiLSTM(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT)
model.embedding.weight.data.copy_(TEXT.vocab.vectors)

# Define loss and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters())

# Define training and evaluation functions
def train(model, iterator, optimizer, criterion):
    model.train()
    epoch_loss = 0
    for batch in iterator:
        text, text_lengths = batch.text
        tags = batch.udtags
        optimizer.zero_grad()
        predictions = model(text, text_lengths).view(-1, OUTPUT_DIM)
        tags = tags.view(-1)
        loss = criterion(predictions, tags)
        loss.backward()
        optimizer.step()
        epoch_loss += loss.item()
    return epoch_loss / len(iterator)

def evaluate(model, iterator, criterion):
    model.eval()
    epoch_loss = 0
    with torch.no_grad():
        for batch in iterator:
            text, text_lengths = batch.text
            tags = batch.udtags
            predictions = model(text, text_lengths).view(-1, OUTPUT_DIM)
            tags = tags.view(-1)
            loss = criterion(predictions, tags)
            epoch_loss += loss.item()
    return epoch_loss / len(iterator)

import torch
import os

# Save the best model
def save_checkpoint(state, filename="model_checkpoint.pth.tar"):
    print("=> Saving checkpoint")
    torch.save(state, filename)

# Compute accuracy
def calculate_accuracy(predictions, labels):
    correct = (predictions.argmax(dim=1) == labels).float()
    return correct.mean()

# Early stopping criteria and model training
def train_model(model, train_iterator, valid_iterator, optimizer, criterion, num_epochs, model_path='best_model.pt'):
    best_valid_accuracy = 0.0
    for epoch in range(num_epochs):
        model.train()
        epoch_train_loss = 0.0
        epoch_train_acc = 0.0
        for batch in train_iterator:
            optimizer.zero_grad()
            text, text_lengths = batch.text
            tags = batch.udtags
            predictions = model(text, text_lengths).view(-1, OUTPUT_DIM)
            tags = tags.view(-1)
            loss = criterion(predictions, tags)
            acc = calculate_accuracy(predictions, tags)
            loss.backward()
            optimizer.step()
            epoch_train_loss += loss.item()
            epoch_train_acc += acc.item()

        train_loss = epoch_train_loss / len(train_iterator)
        train_accuracy = epoch_train_acc / len(train_iterator)

        model.eval()
        epoch_valid_loss = 0.0
        epoch_valid_acc = 0.0
        with torch.no_grad():
            for batch in valid_iterator:
                text, text_lengths = batch.text
                tags = batch.udtags
                predictions = model(text, text_lengths).view(-1, OUTPUT_DIM)
                tags = tags.view(-1)
                loss = criterion(predictions, tags)
                acc = calculate_accuracy(predictions, tags)
                epoch_valid_loss += loss.item()
                epoch_valid_acc += acc.item()

        valid_loss = epoch_valid_loss / len(valid_iterator)
        valid_accuracy = epoch_valid_acc / len(valid_iterator)

        print(f'Epoch: {epoch+1:02}')
        print(f'\tTrain Loss: {train_loss:.3f}, Accuracy: {train_accuracy:.3f}')
        print(f'\t Val. Loss: {valid_loss:.3f}, Accuracy: {valid_accuracy:.3f}')

        if valid_accuracy > best_valid_accuracy:
            best_valid_accuracy = valid_accuracy
            torch.save(model.state_dict(), model_path)
            save_checkpoint({'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict()})

    print("Training completed.")

NUM_EPOCHS = 10
model_path = 'best_model.pt'
train_model(model, train_iterator, valid_iterator, optimizer, criterion, NUM_EPOCHS, model_path)

def calculate_accuracy(predictions, y):
    max_preds = predictions.argmax(dim=1, keepdim=True)  # get the index of the max probability
    correct = max_preds.squeeze(1).eq(y)
    return correct.sum() / torch.FloatTensor([y.shape[0]])

def evaluate_test(model, iterator, criterion):
    model.eval()
    epoch_loss = 0
    epoch_acc = 0

    with torch.no_grad():
        for batch in iterator:
            text, text_lengths = batch.text
            tags = batch.udtags

            predictions = model(text, text_lengths).view(-1, OUTPUT_DIM)
            tags = tags.view(-1)

            loss = criterion(predictions, tags)
            acc = calculate_accuracy(predictions, tags)

            epoch_loss += loss.item()
            epoch_acc += acc.item()

    return epoch_loss / len(iterator), epoch_acc / len(iterator)

# Load the best saved model
model.load_state_dict(torch.load('best_model.pt'))

test_loss, test_acc = evaluate_test(model, test_iterator, criterion)
print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')

import torch

def tag_sentence(sentence, model, vocabulary, tag_index):
    # Tokenize the sentence; here we use a simple split for tokenization
    tokens = sentence.split()

    # Convert tokens to tensor of indices using the vocabulary
    token_indices = [vocabulary.get(token, vocabulary['<unk>']) for token in tokens]
    token_tensor = torch.LongTensor(token_indices).unsqueeze(1)  # Add batch dimension

    # Compute the lengths for packed padded sequences
    text_lengths = torch.LongTensor([len(token_indices)])  # Length of the sequence

    # Model Prediction
    model.eval()  # Set the model to evaluation mode
    with torch.no_grad():
        predictions = model(token_tensor, text_lengths)

    # Ensure predictions are properly shaped
    if predictions.dim() > 1:
        predictions = predictions.squeeze(0)

    # Flatten predictions if nested
    if isinstance(predictions, list) and all(isinstance(pred, list) for pred in predictions):
        predictions = [item for sublist in predictions for item in sublist]

    # Get the indices of the max log-probability
    predicted_indices = predictions.argmax(dim=1).tolist()

    # Debugging: Logging structure of predicted_indices
    print(f"Predicted indices type: {type(predicted_indices)}")
    print(f"Predicted indices content: {predicted_indices}")

    # Convert prediction indices to tags
    predicted_tags = [tag_index.get(idx, 'UNK') for idx in predicted_indices]

    # Form the output sentence with tags
    tagged_sentence = ' '.join([f"{word}/{tag}" for word, tag in zip(tokens, predicted_tags)])

    return tagged_sentence

# Assuming 'model' is already created and trained, and available in the context
# Example usage of the function
sentences = [
    "The old man the boat.",
    "The complex houses married and single soldiers and their families.",
    "The man who hunts ducks out on weekends."
]

for sentence in sentences:
    print(tag_sentence(sentence, model, vocab, tag_index))